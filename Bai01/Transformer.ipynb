{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnZ6QfVSATJG",
        "outputId": "142fac20-1f93-4eb0-c275-39729725c9dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k8eh16uWLtGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# TRANSFORMER CONFIG (COLAB CELL)\n",
        "# ================================\n",
        "\n",
        "# -------- Paths --------\n",
        "DATA_DIR = \"/content/drive/MyDrive/TransformerMT/DATA\"\n",
        "CHECKPOINT_DIR = \"/content/drive/MyDrive/TransformerMT/checkpoints\"\n",
        "\n",
        "TRAIN_EN = f\"{DATA_DIR}/train.en\"\n",
        "TRAIN_VI = f\"{DATA_DIR}/train.vi\"\n",
        "VALID_EN = f\"{DATA_DIR}/valid.en\"\n",
        "VALID_VI = f\"{DATA_DIR}/valid.vi\"\n",
        "\n",
        "VOCAB_EN_PATH = f\"{DATA_DIR}/vocab_en.pth\"\n",
        "VOCAB_VI_PATH = f\"{DATA_DIR}/vocab_vi.pth\"\n",
        "\n",
        "# -------- Special Tokens --------\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "BOS_TOKEN = \"<bos>\"\n",
        "EOS_TOKEN = \"<eos>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "PAD_IDX = 0\n",
        "BOS_IDX = 1\n",
        "EOS_IDX = 2\n",
        "UNK_IDX = 3\n",
        "\n",
        "# -------- Model Hyperparameters --------\n",
        "D_MODEL = 512\n",
        "N_HEADS = 8\n",
        "N_LAYERS = 6\n",
        "D_FF = 2048\n",
        "DROPOUT_RATE = 0.1\n",
        "\n",
        "# -------- Training Hyperparameters --------\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 150\n",
        "\n",
        "LR = 1e-4\n",
        "BETAS = (0.9, 0.98)\n",
        "EPS = 1e-9\n",
        "\n",
        "LABEL_SMOOTHING = 0.1\n",
        "GRAD_CLIP = 1.0\n",
        "WARMUP_STEPS = 4000\n",
        "\n",
        "# -------- Sequence --------\n",
        "MAX_LEN = 128\n",
        "\n",
        "# -------- Device --------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------- Logging / Saving --------\n",
        "LOG_INTERVAL = 100\n",
        "SAVE_EVERY_EPOCH = 1\n",
        "\n",
        "VOCAB_SIZE_SPM=10000\n",
        "MODEL_TYPE = 'bpe'\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "MAX_SEQ_LEN = 128\n",
        "VAL_SPLIT_RATIO = 0.1\n",
        "SEED = 42\n",
        "TEST_MODE_LIMIT=None\n",
        "MAX_LEN_DECODE = 128\n",
        "BEAM_SIZE = 5\n",
        "TEST_EN_PATH = os.path.join(DATA_DIR, 'tst2013.en')\n",
        "TEST_VI_PATH = os.path.join(DATA_DIR, 'tst2013.vi')\n",
        "METRIC_LOG_PATH = os.path.join(DATA_DIR, 'training_log.json')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2yk_nVFS7Pyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import sentencepiece as spm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. CÁC HÀM TIỆN ÍCH (LOAD & STATS)\n",
        "# ==============================================================================\n",
        "\n",
        "def load_raw_data(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Không tìm thấy file: {file_path}\")\n",
        "    print(f\"-> Đang đọc file: {os.path.basename(file_path)}...\")\n",
        "    with open(file_path, encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f]\n",
        "\n",
        "def plot_statistics(raw_data, sp_processor, lang_name):\n",
        "    \"\"\"\n",
        "    Thống kê độ dài câu dựa trên số lượng Subword (Token IDs).\n",
        "    Giúp quyết định MAX_SEQ_LEN hợp lý.\n",
        "    \"\"\"\n",
        "    # Lấy mẫu tối đa 50k câu để thống kê cho nhanh\n",
        "    sample_data = raw_data[:50000] if len(raw_data) > 50000 else raw_data\n",
        "\n",
        "    # Tokenize để đếm số lượng subword thực tế\n",
        "    lengths = [len(sp_processor.encode_as_ids(s.lower())) for s in sample_data]\n",
        "\n",
        "    if not lengths:\n",
        "        print(f\"Dữ liệu {lang_name} trống!\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n[Thống kê {lang_name} (trên {len(sample_data)} mẫu)]\")\n",
        "    print(f\"- Độ dài trung bình: {np.mean(lengths):.2f} subwords\")\n",
        "    print(f\"- Max length: {np.max(lengths)} subwords\")\n",
        "    print(f\"- 95% dữ liệu ngắn hơn: {np.percentile(lengths, 95):.0f} subwords\")\n",
        "    print(f\"- 99% dữ liệu ngắn hơn: {np.percentile(lengths, 99):.0f} subwords\")\n",
        "\n",
        "    # Vẽ biểu đồ\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.hist(lengths, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "    plt.title(f\"Phân phối độ dài Subword ({lang_name})\")\n",
        "    plt.xlabel(\"Số lượng Subword\")\n",
        "    plt.ylabel(\"Số lượng câu\")\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. DATASET & COLLATE FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "class TranslationDatasetSPM(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset thực hiện:\n",
        "    1. Nhận raw text.\n",
        "    2. Tokenize bằng SentencePiece.\n",
        "    3. Lọc bỏ câu quá dài (theo Subword Count).\n",
        "    4. Thêm <sos> và <eos>.\n",
        "    \"\"\"\n",
        "    def __init__(self, raw_en, raw_vi, sp_en, sp_vi, max_len=100):\n",
        "        self.sp_en = sp_en\n",
        "        self.sp_vi = sp_vi\n",
        "        self.SOS_IDX = sp_en.bos_id()\n",
        "        self.EOS_IDX = sp_en.eos_id()\n",
        "\n",
        "        self.data = []\n",
        "        original_count = len(raw_en)\n",
        "\n",
        "        print(f\"-> Đang xử lý và lọc {original_count} cặp câu (Max Len={max_len})...\")\n",
        "\n",
        "        # Duyệt qua từng cặp câu\n",
        "        for en, vi in zip(raw_en, raw_vi):\n",
        "            # 1. Encode sang IDs (lowercase để chuẩn hóa)\n",
        "            en_ids = self.sp_en.encode_as_ids(en.lower())\n",
        "            vi_ids = self.sp_vi.encode_as_ids(vi.lower())\n",
        "\n",
        "            # 2. Kiểm tra độ dài (Tính cả SOS và EOS sẽ thêm vào)\n",
        "            if len(en_ids) + 2 <= max_len and len(vi_ids) + 2 <= max_len:\n",
        "                self.data.append((en_ids, vi_ids))\n",
        "\n",
        "        filtered_count = len(self.data)\n",
        "        removed_count = original_count - filtered_count\n",
        "        print(f\"-> Hoàn tất. Giữ lại: {filtered_count} | Loại bỏ: {removed_count} ({removed_count/original_count:.2%})\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        en_ids, vi_ids = self.data[index]\n",
        "\n",
        "        # Thêm SOS đầu và EOS cuối\n",
        "        en_out = [self.SOS_IDX] + en_ids + [self.EOS_IDX]\n",
        "        vi_out = [self.SOS_IDX] + vi_ids + [self.EOS_IDX]\n",
        "\n",
        "        # Trả về LongTensor\n",
        "        return torch.tensor(en_out, dtype=torch.long), torch.tensor(vi_out, dtype=torch.long)\n",
        "\n",
        "class MyCollateSPM:\n",
        "    \"\"\"\n",
        "    Hàm gom batch: Padding các câu trong batch cho bằng nhau.\n",
        "    \"\"\"\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        # Tách src và trg từ batch list\n",
        "        src = [item[0] for item in batch]\n",
        "        trg = [item[1] for item in batch]\n",
        "\n",
        "        # Padding (batch_first=True -> Output: (Batch, Seq_Len))\n",
        "        src = pad_sequence(src, batch_first=True, padding_value=self.pad_idx)\n",
        "        trg = pad_sequence(trg, batch_first=True, padding_value=self.pad_idx)\n",
        "\n",
        "        return src, trg\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. LUỒNG CHẠY CHÍNH (MAIN DATA PIPELINE)\n",
        "# ==============================================================================\n",
        "\n",
        "def process_data_pipeline():\n",
        "    print(\"=== BẮT ĐẦU QUY TRÌNH XỬ LÝ DỮ LIỆU ===\\n\")\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # BƯỚC 1: LOAD DỮ LIỆU THÔ\n",
        "    # -----------------------------------------\n",
        "    path_en = os.path.join(DATA_DIR, 'train.en')\n",
        "    path_vi = os.path.join(DATA_DIR, 'train.vi')\n",
        "\n",
        "    raw_en = load_raw_data(path_en)\n",
        "    raw_vi = load_raw_data(path_vi)\n",
        "\n",
        "    assert len(raw_en) == len(raw_vi), \"Lỗi: Số lượng câu Anh-Việt không khớp!\"\n",
        "    print(f\"-> Tổng số câu raw: {len(raw_en)}\")\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # BƯỚC 2: CHIA TẬP TRAIN / VAL\n",
        "    # -----------------------------------------\n",
        "    print(f\"\\n--- Chia tập dữ liệu (Val Ratio: {VAL_SPLIT_RATIO}) ---\")\n",
        "    en_train, en_val, vi_train, vi_val = train_test_split(\n",
        "        raw_en, raw_vi, test_size=VAL_SPLIT_RATIO, random_state=SEED\n",
        "    )\n",
        "    print(f\"-> Train set: {len(en_train)} câu\")\n",
        "    print(f\"-> Val set:   {len(en_val)} câu\")\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # BƯỚC 3: LOAD MODEL SENTENCEPIECE (ĐÃ TRAIN SẴN)\n",
        "    # -----------------------------------------\n",
        "    print(\"\\n--- Load SentencePiece Models ---\")\n",
        "    # Giả định file model tên là spm_en.model và spm_vi.model\n",
        "    sp_en_path = os.path.join(DATA_DIR, 'spm_en.model')\n",
        "    sp_vi_path = os.path.join(DATA_DIR, 'spm_vi.model')\n",
        "\n",
        "    if not os.path.exists(sp_en_path) or not os.path.exists(sp_vi_path):\n",
        "        raise FileNotFoundError(\"Chưa tìm thấy file model SPM (.model). Hãy đảm bảo bạn đã train SPM trước đó.\")\n",
        "\n",
        "    sp_en = spm.SentencePieceProcessor()\n",
        "    sp_en.load(sp_en_path)\n",
        "\n",
        "    sp_vi = spm.SentencePieceProcessor()\n",
        "    sp_vi.load(sp_vi_path)\n",
        "\n",
        "    PAD_IDX = sp_en.pad_id() # Thường là 0\n",
        "    print(f\"-> Đã load SPM. Vocab EN: {sp_en.get_piece_size()} | Vocab VI: {sp_vi.get_piece_size()}\")\n",
        "    print(f\"-> PAD_IDX: {PAD_IDX}\")\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # BƯỚC 4: THỐNG KÊ (Dựa trên Subword)\n",
        "    # -----------------------------------------\n",
        "    print(\"\\n--- Thống kê Dữ liệu Train (Subword Level) ---\")\n",
        "    plot_statistics(en_train, sp_en, \"English (Source)\")\n",
        "    plot_statistics(vi_train, sp_vi, \"Vietnamese (Target)\")\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # BƯỚC 5: TẠO DATASET & DATALOADER\n",
        "    # -----------------------------------------\n",
        "    print(\"\\n--- Tạo Dataset và DataLoader ---\")\n",
        "\n",
        "    # Tạo Dataset (Tokenize & Filter)\n",
        "    train_dataset = TranslationDatasetSPM(en_train, vi_train, sp_en, sp_vi, max_len=MAX_SEQ_LEN)\n",
        "    val_dataset = TranslationDatasetSPM(en_val, vi_val, sp_en, sp_vi, max_len=MAX_SEQ_LEN)\n",
        "\n",
        "    # Collate Function\n",
        "    collate_fn = MyCollateSPM(pad_idx=PAD_IDX)\n",
        "\n",
        "    # Tạo DataLoader\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2,    # Tăng tốc độ load data (tuỳ chọn)\n",
        "        pin_memory=True   # Tối ưu khi chuyển sang GPU\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\n-> Train Loader: {len(train_loader)} batches\")\n",
        "    print(f\"-> Val Loader:   {len(val_loader)} batches\")\n",
        "\n",
        "    # Kiểm tra thử 1 batch\n",
        "    src_batch, trg_batch = next(iter(train_loader))\n",
        "    print(f\"\\n[Test Batch Shape]\")\n",
        "    print(f\"Src: {src_batch.shape} (Batch, Seq_len)\")\n",
        "    print(f\"Trg: {trg_batch.shape}\")\n",
        "\n",
        "    return train_loader, val_loader, sp_en, sp_vi\n",
        "\n",
        "# --- CHẠY CHƯƠNG TRÌNH ---\n",
        "if __name__ == \"__main__\":\n",
        "    #train_dl, val_dl, sp_en, sp_vi = process_data_pipeline()\n",
        "    print(\"\\n>>> XỬ LÝ DỮ LIỆU HOÀN TẤT. SẴN SÀNG TRAIN MODEL <<<\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyDcaGlt0kmL",
        "outputId": "14c5be14-c522-4503-856f-f9ad374c09b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> XỬ LÝ DỮ LIỆU HOÀN TẤT. SẴN SÀNG TRAIN MODEL <<<\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp_en = spm.SentencePieceProcessor()\n",
        "sp_en.load(os.path.join(DATA_DIR, 'spm_en.model'))\n",
        "\n",
        "sp_vi = spm.SentencePieceProcessor()\n",
        "sp_vi.load(os.path.join(DATA_DIR, 'spm_vi.model'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr_eWn00Rnr1",
        "outputId": "90b96fb4-d465-45dd-a903-ec0c892e41dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaled Dot-Product Attention & Multi-Head Attention"
      ],
      "metadata": {
        "id": "sqobrHDz4uYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
        "    d_k = query.size(-1)\n",
        "\n",
        "    # scores shape: (Batch_size, Num_Heads, Seq_len_Q, Seq_len_K)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    # 1. Áp dụng Mask\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # 2. Tính Softmax\n",
        "    p_attn = torch.softmax(scores, dim=-1)\n",
        "\n",
        "    # 3. ÁP DỤNG DROPOUT\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "\n",
        "    # 4. Tính đầu ra\n",
        "    output = torch.matmul(p_attn, value)\n",
        "\n",
        "    return output, p_attn"
      ],
      "metadata": {
        "id": "zaPsLmRJzkj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout_rate=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model phải chia hết cho num_heads\"\n",
        "\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # 4 lớp Linear: Wq, Wk, Wv, và Wo (output)\n",
        "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        # Dropout Module\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        if mask is not None and mask.dim() == 3:\n",
        "            # (B, L, L) → (B, 1, L, L)\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        # 1. Ánh xạ tuyến tính và tách thành Heads\n",
        "        def transform(x, w):\n",
        "            return w(x).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        q = transform(query, self.w_q)\n",
        "        k = transform(key, self.w_k)\n",
        "        v = transform(value, self.w_v)\n",
        "\n",
        "        # 2. Tính Scaled Dot-Product Attention (truyền module dropout vào)\n",
        "        # Bắt buộc phải truyền self.dropout để gọi hàm forward() của nn.Dropout\n",
        "        x, self_attn = scaled_dot_product_attention(q, k, v, mask=mask, dropout=self.dropout)\n",
        "\n",
        "        # 3. Nối các đầu (Concatenate Heads)\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "        # 4. Ánh xạ tuyến tính cuối cùng (Wo)\n",
        "        return self.w_o(x), self_attn"
      ],
      "metadata": {
        "id": "ndbT5OTg5Eqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding (Sinusoidal):"
      ],
      "metadata": {
        "id": "Op9CJa5V-YAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional Encoding (PE) Sinusoidal theo công thức gốc của bài báo \"Attention Is All You Need\".\n",
        "    PE là một ma trận cố định, không học được.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, dropout_rate, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        # Tạo ma trận PE (Max_Len, D_model)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "\n",
        "        # 1. Tính Pos (Vị trí): (Max_Len, 1)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "\n",
        "        # 2. Tính Div_term (Mẫu số): 10000^(2i/d_model)\n",
        "        # Sử dụng log và exp để tránh lỗi số học: exp(2i * -log(10000) / d_model)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        # 3. Gán giá trị Sin/Cos\n",
        "        # Cột chẵn (2i) là Sin:\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Cột lẻ (2i+1) là Cos:\n",
        "        # Nếu d_model lẻ, cột cuối cùng sẽ được gán bằng pe[:, d_model-1]\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Thêm chiều batch (1, Max_Len, D_model)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Đăng ký là buffer (không tham gia vào quá trình backward/gradient descent)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Input: x (Embedding đầu vào) shape: (Batch_size, Seq_len, D_model)\n",
        "        Output: x + PE\n",
        "        \"\"\"\n",
        "        # Thêm Positional Encoding vào Embedding.\n",
        "        # Ta chỉ lấy PE đến độ dài Seq_len của batch hiện tại.\n",
        "        # Kích thước của x là x.size(1)\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "\n",
        "        # Áp dụng Dropout\n",
        "        return self.dropout(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "Sr7TP3IS-i_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Encoder Layer"
      ],
      "metadata": {
        "id": "2ej2chUC_wbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Lớp Feed-Forward Network (FFN) trong Transformer.\n",
        "    Áp dụng riêng rẽ (position-wise) cho từng vị trí trong chuỗi.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff, dropout_rate=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "\n",
        "        # Lớp 1: Tăng chiều (d_model -> d_ff)\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "\n",
        "        # Lớp 2: Giảm chiều (d_ff -> d_model)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (Batch_size, Seq_len, D_model)\n",
        "\n",
        "        # 1. Linear 1 -> ReLU\n",
        "        # Output shape: (Batch_size, Seq_len, D_ff)\n",
        "        output = F.relu(self.w_1(x))\n",
        "\n",
        "        # 2. Dropout\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        # 3. Linear 2\n",
        "        # Output shape: (Batch_size, Seq_len, D_model)\n",
        "        return self.w_2(output)"
      ],
      "metadata": {
        "id": "wbrbZYh-_vBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    def __init__(self, size, dropout_rate=0.1):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "\n",
        "        # size chính là d_model (ví dụ: 512)\n",
        "        self.norm = nn.LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"\"\"\n",
        "        Input: x (input của sublayer), sublayer (hàm/module của sublayer: MHA hoặc FFN)\n",
        "        \"\"\"\n",
        "        # 1. Layer Normalization\n",
        "        norm_x = self.norm(x)\n",
        "\n",
        "        # 2. Tính Sublayer (MHA hoặc FFN)\n",
        "        sublayer_output = sublayer(norm_x)\n",
        "\n",
        "        # 3. Dropout\n",
        "        sublayer_output = self.dropout(sublayer_output)\n",
        "\n",
        "        # 4. Residual Connection (Add)\n",
        "        return x + sublayer_output\n"
      ],
      "metadata": {
        "id": "7yeOhWo0Ckkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Một lớp đơn lẻ trong Transformer Encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout_rate):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn        # MultiHeadAttention\n",
        "        self.feed_forward = feed_forward  # PositionwiseFeedForward\n",
        "        self.sublayer = nn.ModuleList([\n",
        "            SublayerConnection(size, dropout_rate), # Cho Self-Attention\n",
        "            SublayerConnection(size, dropout_rate)  # Cho FFN\n",
        "        ])\n",
        "        self.size = size # d_model\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x shape: (Batch_size, Seq_len, D_model)\n",
        "\n",
        "        # 1. Sublayer 1: Multi-Head Self-Attention (với Residual & Norm)\n",
        "        # Tự chú ý (Self-Attention): Q=K=V=x. Encoder không cần look-ahead mask.\n",
        "        # Mask ở đây là Padding Mask (Mask: 1 là giữ, 0 là ẩn)\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask=mask)[0])\n",
        "\n",
        "        # 2. Sublayer 2: Feed-Forward Network (với Residual & Norm)\n",
        "        x = self.sublayer[1](x, self.feed_forward)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "503ZzcewCpQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Decoder Layer"
      ],
      "metadata": {
        "id": "o0cgyiqRGMh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    return torch.tril(torch.ones(1, size, size, dtype=torch.bool))\n",
        "\n",
        "def make_std_mask(tgt, pad_idx):\n",
        "    \"\"\"\n",
        "    Tạo Target Mask (kết hợp Padding Mask và Look-Ahead Mask).\n",
        "    Dùng cho Decoder Self-Attention.\n",
        "    \"\"\"\n",
        "    # 1. Tạo Padding Mask (True/1 cho vị trí KHÔNG phải PAD)\n",
        "    # unsqueeze(-2) để kích thước là (B, 1, L_tgt)\n",
        "    tgt_pad_mask = (tgt != pad_idx).unsqueeze(-2)\n",
        "\n",
        "    # 2. Tạo Look-Ahead Mask\n",
        "    seq_len = tgt.size(-1)\n",
        "    look_ahead_mask = subsequent_mask(seq_len).type_as(tgt_pad_mask.data)\n",
        "\n",
        "    # 3. Kết hợp (AND logic): Chỉ True nếu KHÔNG phải PAD VÀ KHÔNG phải tương lai\n",
        "    tgt_mask = tgt_pad_mask & look_ahead_mask\n",
        "\n",
        "    return tgt_mask.requires_grad_(False)\n",
        "\n",
        "# =================================================================\n",
        "# PHẦN B: LỚP BATCH - TỔ CHỨC INPUT VÀ TẠO MASKS CHO MỖI LẦN CHẠY\n",
        "# =================================================================\n",
        "\n",
        "class Batch:\n",
        "    \"\"\"\n",
        "    Lớp đóng gói dữ liệu đầu vào (src/tgt) và tạo ra tất cả các masks cần thiết.\n",
        "    \"\"\"\n",
        "    def __init__(self, src, tgt, pad_idx):\n",
        "        self.src = src # (B, L_src)\n",
        "\n",
        "        # 1. Tách Target cho Input và Output\n",
        "        # TGT INPUT: Dùng để đưa vào Decoder (gồm <sos>...tok_n)\n",
        "        self.tgt = tgt[:, :-1]\n",
        "        # TGT OUTPUT (tgt_y): Dùng để tính Loss (gồm tok_1...<eos>)\n",
        "        self.tgt_y = tgt[:, 1:]\n",
        "\n",
        "        # 2. TẠO MASKS CẦN THIẾT\n",
        "\n",
        "        # a) Source Mask (Padding Mask): Dùng cho ENCODER & CROSS-ATTENTION\n",
        "        # Kích thước (B, 1, L_src)\n",
        "        self.src_mask = (src != pad_idx).unsqueeze(-2).requires_grad_(False)\n",
        "\n",
        "        # b) Target Mask (Padding + Look-Ahead): Dùng cho DECODER SELF-ATTENTION\n",
        "        # Kích thước (B, L_tgt, L_tgt)\n",
        "        self.tgt_mask = make_std_mask(self.tgt, pad_idx)\n",
        "\n",
        "        # Số lượng token thực tế (không phải pad) để chuẩn hóa Loss\n",
        "        self.ntokens = (self.tgt_y != pad_idx).data.sum()\n"
      ],
      "metadata": {
        "id": "XOJyEzKcGQj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eQLJkJPnwsRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Một lớp đơn lẻ trong Transformer Decoder.\n",
        "    Chứa: Masked Self-Attention, Cross-Attention, và FFN.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout_rate):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn      # MultiHeadAttention (Cho Self-Attention)\n",
        "        self.src_attn = src_attn        # MultiHeadAttention (Cho Cross-Attention)\n",
        "        self.feed_forward = feed_forward\n",
        "\n",
        "        # 3 Sublayer Connections cho 3 Sublayer\n",
        "        self.sublayer = nn.ModuleList([\n",
        "            SublayerConnection(size, dropout_rate), # Masked Self-Attention\n",
        "            SublayerConnection(size, dropout_rate), # Cross-Attention\n",
        "            SublayerConnection(size, dropout_rate)  # FFN\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          x: decoder input (output của lớp trước) (Batch_size, L_tgt, D_model)\n",
        "          memory: output của Encoder (Batch_size, L_src, D_model)\n",
        "          src_mask: Padding Mask cho Encoder Output (memory)\n",
        "          tgt_mask: Look-Ahead Mask (Subsequent Mask) cho Decoder Input (x)\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Sublayer 1: Masked Multi-Head Self-Attention\n",
        "        # Q=K=V=x. Dùng tgt_mask (Look-Ahead Mask)\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask=tgt_mask)[0])\n",
        "\n",
        "        # 2. Sublayer 2: Multi-Head Cross-Attention (Encoder-Decoder Attention)\n",
        "        # Q=x (target), K=V=memory (source/encoder output)\n",
        "        # Dùng src_mask (Padding Mask) để không chú ý đến các token <pad> từ Encoder\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, memory, memory, mask=src_mask)[0])\n",
        "\n",
        "        # 3. Sublayer 3: Feed-Forward Network\n",
        "        x = self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "SsFihiN0GcZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer"
      ],
      "metadata": {
        "id": "Zn80JdxFPIgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Toàn bộ Transformer Encoder, bao gồm Stacking N Encoder Layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        # layer: là một đối tượng EncoderLayer đã được khởi tạo\n",
        "        # N: số lượng lớp (ví dụ N=6)\n",
        "\n",
        "        # Stacking N lớp\n",
        "        self.layers = clones(layer, N)\n",
        "\n",
        "        # Layer Normalization cuối cùng\n",
        "        self.norm = nn.LayerNorm(layer.size)\n",
        "\n",
        "        self.size = layer.size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # x shape: (Batch_size, Seq_len, D_model) - Đã là Embedding + PE\n",
        "        # mask: Padding Mask (từ Source)\n",
        "\n",
        "        # Chạy qua N lớp Encoder Layer\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        # Áp dụng Layer Norm cuối cùng trước khi trả về\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "eNB2S7qCMcu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Toàn bộ Transformer Decoder, bao gồm Stacking N Decoder Layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        # layer: là một đối tượng DecoderLayer đã được khởi tạo\n",
        "\n",
        "        # Stacking N lớp\n",
        "        self.layers = clones(layer, N)\n",
        "\n",
        "        # Layer Normalization cuối cùng\n",
        "        self.norm = nn.LayerNorm(layer.size)\n",
        "\n",
        "        self.size = layer.size\n",
        "\n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        # x: Decoder Input (Embedding + PE)\n",
        "        # memory: Output của Encoder (Encoder Memory)\n",
        "        # src_mask: Padding Mask của Source\n",
        "        # tgt_mask: Look-Ahead Mask của Target\n",
        "\n",
        "        # Chạy qua N lớp Decoder Layer\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "\n",
        "        # Áp dụng Layer Norm cuối cùng trước khi trả về\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "6Skba5JtPW5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Mô hình Transformer hoàn chỉnh (Source-to-Target)\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = encoder      # Lớp Encoder (Stacking N Encoder Layers)\n",
        "        self.decoder = decoder      # Lớp Decoder (Stacking N Decoder Layers)\n",
        "        self.src_embed = src_embed  # Lớp Embedding + PE cho Source\n",
        "        self.tgt_embed = tgt_embed  # Lớp Embedding + PE cho Target\n",
        "        self.generator = generator  # Lớp Linear cuối cùng (Projection to Vocab)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          src: Source sequence (indices) (B, L_src)\n",
        "          tgt: Target sequence (indices) (B, L_tgt)\n",
        "          src_mask: Padding Mask (B, 1, L_src)\n",
        "          tgt_mask: Look-Ahead Mask (B, L_tgt, L_tgt)\n",
        "        \"\"\"\n",
        "\n",
        "        # 1. Chạy Encoder và lấy Memory\n",
        "        memory = self.encode(src, src_mask) # (B, L_src, D_model)\n",
        "\n",
        "        # 2. Chạy Decoder\n",
        "        output = self.decode(memory, src_mask, tgt, tgt_mask) # (B, L_tgt, D_model)\n",
        "\n",
        "        # 3. Projection Logits (Chuyển D_model thành Vocab Size)\n",
        "        return self.generator(output)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # src_embed(src): Embedding + Positional Encoding\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "\n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        # tgt_embed(tgt): Embedding + Positional Encoding\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "\n",
        "# --- Generator/Projection Layer ---\n",
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab_size):\n",
        "        super(Generator, self).__init__()\n",
        "        # Lớp tuyến tính ánh xạ D_model về Vocab Size\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Trả về Logits (trước Softmax)\n",
        "        return self.proj(x)\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "I8F-uZ4ZPZYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hàm utility clones (rất quan trọng) ---\n",
        "def clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "def make_model(src_vocab, tgt_vocab, N=N_LAYERS, d_model=D_MODEL, d_ff=D_FF, h=N_HEADS, dropout=DROPOUT_RATE):\n",
        "    \"Hàm xây dựng mô hình Transformer từ các khối đã định nghĩa.\"\n",
        "\n",
        "    # 1. Khởi tạo các Sub-module\n",
        "    c = copy.deepcopy\n",
        "\n",
        "    # Giả định các lớp này đã được định nghĩa chi tiết (MHA, FFN, PE, SublayerConnection)\n",
        "    attn = MultiHeadAttention(d_model, h, dropout)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    pe = PositionalEncoding(d_model, dropout)\n",
        "    sublayer_conn = SublayerConnection(d_model, dropout) # Mẫu để clones\n",
        "\n",
        "    # 2. Xây dựng Layer mẫu (sử dụng clones an toàn cho Sublayers)\n",
        "    class SafeEncoderLayer(EncoderLayer):\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.sublayer = clones(sublayer_conn, 2)\n",
        "\n",
        "    class SafeDecoderLayer(DecoderLayer):\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            super().__init__(*args, **kwargs)\n",
        "            self.sublayer = clones(sublayer_conn, 3)\n",
        "\n",
        "    enc_layer_proto = SafeEncoderLayer(d_model, c(attn), c(ff), dropout)\n",
        "    dec_layer_proto = SafeDecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)\n",
        "\n",
        "    # 3. Lắp ráp Encoder/Decoder lớn (dùng clones cho N layers)\n",
        "    encoder = Encoder(enc_layer_proto, N)\n",
        "    decoder = Decoder(dec_layer_proto, N)\n",
        "\n",
        "    # 4. Lớp Embedding và Generator\n",
        "    src_embed = nn.Sequential(Embeddings(d_model, src_vocab), c(pe))\n",
        "    tgt_embed = nn.Sequential(Embeddings(d_model, tgt_vocab), c(pe))\n",
        "    generator = Generator(d_model, tgt_vocab)\n",
        "\n",
        "    # 5. Mô hình Hoàn chỉnh\n",
        "    model = Transformer(\n",
        "        encoder, decoder,\n",
        "        src_embed, tgt_embed,\n",
        "        generator\n",
        "    )\n",
        "\n",
        "    # 6. Khởi tạo tham số bằng Xavier (Giúp ổn định training)\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "UzEom60L4VnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "6BxbfpuW7ZEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "class LossCompute:\n",
        "    def __init__(self, criterion, opt=None, clip_norm=None):\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        self.clip_norm = clip_norm\n",
        "\n",
        "    def __call__(self, x, y, norm):\n",
        "        loss = self.criterion(\n",
        "            x.contiguous().view(-1, x.size(-1)),\n",
        "            y.contiguous().view(-1)\n",
        "        )\n",
        "\n",
        "        normalized_loss = loss / norm\n",
        "\n",
        "        if self.opt is not None:\n",
        "            normalized_loss.backward()\n",
        "\n",
        "            if self.clip_norm is not None:\n",
        "                clip_grad_norm_(\n",
        "                    (p for g in self.opt.optimizer.param_groups for p in g['params']),\n",
        "                    self.clip_norm\n",
        "                )\n",
        "\n",
        "            self.opt.step()\n",
        "            self.opt.zero_grad()\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "3yUF-Ktbg52R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    def __init__(self, size, padding_idx, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.padding_idx = padding_idx\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.denominator = size - 2\n",
        "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        assert logits.size(1) == self.size\n",
        "\n",
        "        log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(log_probs)\n",
        "            true_dist.fill_(self.smoothing / self.denominator)\n",
        "            true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
        "            true_dist[:, self.padding_idx] = 0.0\n",
        "            true_dist[target == self.padding_idx] = 0.0\n",
        "\n",
        "        return self.criterion(log_probs, true_dist)\n"
      ],
      "metadata": {
        "id": "oqCPWuo07YtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "\n",
        "class NoamOpt:\n",
        "    \"Optimizer tùy chỉnh với Noam/Warmup scheduler, hỗ trợ Checkpointing.\"\n",
        "    def __init__(self, model_size, warmup_steps, optimizer):\n",
        "        # Lưu trữ base optimizer (có thể là Adam, AdamW,...)\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Các trạng thái cần lưu\n",
        "        self._step = 0\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "\n",
        "        # Thiết lập LR ban đầu trong param_groups\n",
        "        self.set_lr_in_param_groups(0)\n",
        "\n",
        "    def set_lr_in_param_groups(self, rate):\n",
        "        \"Hàm tiện ích để cập nhật LR cho base optimizer.\"\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "\n",
        "    def step(self):\n",
        "        \"Cập nhật các tham số và tốc độ học.\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "\n",
        "        # Cập nhật LR của base optimizer\n",
        "        self.set_lr_in_param_groups(rate)\n",
        "\n",
        "        # Gọi step của base optimizer\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def rate(self, step=None):\n",
        "        \"Tính toán LR theo công thức Noam.\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        if step == 0:\n",
        "            return 0\n",
        "\n",
        "        # Công thức Noam: d_model^(-0.5) * min(step^(-0.5), step * warmup_steps^(-1.5))\n",
        "        return self.model_size**(-0.5) * \\\n",
        "               min(step**(-0.5), step * self.warmup_steps**(-1.5))\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Gọi zero_grad của base optimizer.\"\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "    # --- PHƯƠNG THỨC HỖ TRỢ CHECKPOINTING (Vấn đề 2) ---\n",
        "    def state_dict(self):\n",
        "        \"Trả về state_dict của Scheduler (bao gồm trạng thái của base optimizer và step).\"\n",
        "        return {\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'step': self._step,\n",
        "            'warmup_steps': self.warmup_steps,\n",
        "            'model_size': self.model_size\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        \"Load trạng thái cho Scheduler.\"\n",
        "        self.optimizer.load_state_dict(state_dict['optimizer'])\n",
        "        self._step = state_dict['step']\n",
        "        self.warmup_steps = state_dict['warmup_steps']\n",
        "        self.model_size = state_dict['model_size']\n",
        "        # Sau khi load, cần cập nhật lại LR hiện tại\n",
        "        self.set_lr_in_param_groups(self.rate())\n"
      ],
      "metadata": {
        "id": "Iznz68Bo4WG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_ppl(val_loader, model, pad_idx, criterion_val):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src_batch, tgt_batch in val_loader:\n",
        "            src_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n",
        "            batch = Batch(src_batch, tgt_batch, pad_idx)\n",
        "\n",
        "            output_logits = model(\n",
        "                batch.src, batch.tgt,\n",
        "                batch.src_mask, batch.tgt_mask\n",
        "            )\n",
        "\n",
        "            loss = criterion_val(\n",
        "                output_logits.view(-1, output_logits.size(-1)),\n",
        "                batch.tgt_y.view(-1)\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += batch.ntokens.item()\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float('inf'), float('inf')\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return avg_loss, ppl\n"
      ],
      "metadata": {
        "id": "zJyneVTbC67c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Hàm này sẽ được gọi từ bên trong run_full_training_pipeline\n",
        "def calculate_ppl(val_loader, model, pad_idx, criterion_val):\n",
        "    \"Tính Perplexity trên Validation Set bằng Cross-Entropy Loss chuẩn.\"\n",
        "    model.eval() # Bắt buộc phải chuyển sang chế độ đánh giá\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src_batch, tgt_batch in val_loader:\n",
        "            src_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n",
        "            batch = Batch(src_batch, tgt_batch, pad_idx)\n",
        "\n",
        "            output_logits = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
        "\n",
        "            # Log-Softmax Logits (Vì criterion_val là CrossEntropyLoss)\n",
        "            log_probs = F.log_softmax(output_logits, dim=-1)\n",
        "\n",
        "            # Tính Loss bằng Cross-Entropy chuẩn (reduction='sum')\n",
        "            loss = criterion_val(log_probs.contiguous().view(-1, output_logits.size(-1)),\n",
        "                                 batch.tgt_y.contiguous().view(-1))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += batch.ntokens.item()\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float('inf'), float('inf')\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    return avg_loss, perplexity"
      ],
      "metadata": {
        "id": "y04FNWwUDgBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# --- CÁC THAM SỐ CHUNG VÀ BIẾN GLOBAL (Giả định) ---\n",
        "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# PAD_IDX = 0\n",
        "# N_EPOCHS = 10\n",
        "# CHECKPOINT_DIR = '/content/drive/MyDrive/TransformerMT/checkpoints'\n",
        "# Giả định: model, train_loader, val_loader, criterion, optimizer đã được khởi tạo\n",
        "\n",
        "# --- CÁC HÀM TIỆN ÍCH CẦN THIẾT ---\n",
        "# Batch, calculate_ppl, LossCompute, NoamOpt đã được định nghĩa và sửa đổi\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, best_val_ppl,\n",
        "                    is_best=False, filename='latest_checkpoint.pth'):\n",
        "\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_val_ppl': best_val_ppl,\n",
        "        'config': {\n",
        "            'pad_idx': PAD_IDX,\n",
        "            'n_epochs': EPOCHS,\n",
        "            'clip_norm': GRAD_CLIP\n",
        "        }\n",
        "    }\n",
        "\n",
        "    path = os.path.join(CHECKPOINT_DIR, filename)\n",
        "    torch.save(state, path)\n",
        "\n",
        "    if is_best:\n",
        "        torch.save(state, os.path.join(CHECKPOINT_DIR, 'best_model.pth'))\n",
        "        print(f\"🔥 Best model saved at epoch {epoch}\")\n",
        "\n",
        "    print(f\"💾 Checkpoint saved: {path}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename='latest_checkpoint.pth'):\n",
        "    \"\"\"Tải trạng thái mô hình và optimizer để tiếp tục training.\"\"\"\n",
        "    filepath = os.path.join(CHECKPOINT_DIR, filename)\n",
        "\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Không tìm thấy checkpoint tại {filepath}. Bắt đầu training từ đầu.\")\n",
        "        return 1, float('inf') # Trả về epoch = 1 và PPL vô cực\n",
        "\n",
        "    print(f\"-> Đang tải checkpoint từ {filepath}...\")\n",
        "\n",
        "    checkpoint = torch.load(filepath, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    best_val_ppl = checkpoint['best_val_ppl']\n",
        "\n",
        "    print(f\"-> Tải thành công. Tiếp tục training từ Epoch {start_epoch}, PPL tốt nhất: {best_val_ppl:.2f}\")\n",
        "\n",
        "    return start_epoch, best_val_ppl\n",
        "\n",
        "\n",
        "def run_full_training_pipeline(model, train_loader, val_loader,\n",
        "                               criterion_train, criterion_val, # <-- ĐÃ SỬA CHỮ KÝ\n",
        "                               optimizer, pad_idx, n_epochs, clip_norm=1.0):\n",
        "\n",
        "    # 1. TẢI CHECKPOINT\n",
        "    start_epoch, best_val_ppl = load_checkpoint(model, optimizer)\n",
        "\n",
        "    # Khởi tạo Loss Compute (Sử dụng criterion_train cho Forward/Backward)\n",
        "    loss_compute = LossCompute(criterion_train, optimizer, clip_norm=clip_norm)\n",
        "\n",
        "    for epoch in range(start_epoch, n_epochs + 1):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        # Tích lũy Loss và Tokens dưới dạng Tensor trên GPU (Để tối ưu hiệu năng)\n",
        "        total_loss = torch.tensor(0.0, device=DEVICE)\n",
        "        total_tokens = torch.tensor(0, device=DEVICE)\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{n_epochs} (Train)\", unit=\"batch\")\n",
        "\n",
        "        for i, (src_batch, tgt_batch) in enumerate(progress_bar):\n",
        "\n",
        "            src_batch, tgt_batch = src_batch.to(DEVICE), tgt_batch.to(DEVICE)\n",
        "            batch = Batch(src_batch, tgt_batch, pad_idx)\n",
        "\n",
        "            # FORWARD PASS VÀ BACKWARD\n",
        "            # Lưu ý: loss_compute sử dụng criterion_train (Label Smoothing)\n",
        "            output_logits = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
        "            loss_value = loss_compute(output_logits, batch.tgt_y, batch.ntokens)\n",
        "\n",
        "            # Tích lũy trên GPU\n",
        "            total_loss += loss_value\n",
        "            total_tokens += batch.ntokens\n",
        "\n",
        "            # LOGGING\n",
        "            if total_tokens.item() > 0:\n",
        "                avg_loss = total_loss.item() / total_tokens.item()\n",
        "\n",
        "                progress_bar.set_postfix(\n",
        "                    Loss=f\"{avg_loss:.4f}\",\n",
        "                    PPL=f\"{math.exp(avg_loss):.2f}\",\n",
        "                    LR=f\"{optimizer.rate():.6f}\"\n",
        "                )\n",
        "\n",
        "        # --- ĐÁNH GIÁ TRÊN VALIDATION SET ---\n",
        "        # SỬ DỤNG CRITERION_VAL (Cross-Entropy chuẩn) để đo lường chính xác\n",
        "        val_loss, val_ppl = calculate_ppl(val_loader, model, pad_idx, criterion_val)\n",
        "\n",
        "        # ... (Phần in log và lưu checkpoint giữ nguyên)\n",
        "        print(f\"\\n===========================================================\")\n",
        "        print(f\"EPOCH {epoch} KẾT THÚC | TRAIN LOSS: {avg_loss:.4f} | VAL LOSS: {val_loss:.4f}\")\n",
        "        print(f\"VAL PERPLEXITY (PPL): {val_ppl:.2f}\")\n",
        "        print(f\"===========================================================\\n\")\n",
        "\n",
        "        # LƯU CHECKPOINT (Logic tương tự)\n",
        "        is_best = val_ppl < best_val_ppl\n",
        "        if is_best:\n",
        "            best_val_ppl = val_ppl\n",
        "\n",
        "        save_checkpoint(model, optimizer, epoch, best_val_ppl, is_best=is_best)\n",
        "\n",
        "        model.train()"
      ],
      "metadata": {
        "id": "FR6V_reFHKsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CTt13RdMNLm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_pipeline_for_test():\n",
        "    print(\"=== BẮT ĐẦU XỬ LÝ DỮ LIỆU (TEST MODE) ===\\n\")\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # BƯỚC 1: LOAD DỮ LIỆU THÔ\n",
        "    # -----------------------------------------\n",
        "    path_en = os.path.join(DATA_DIR, 'train.en')\n",
        "    path_vi = os.path.join(DATA_DIR, 'train.vi')\n",
        "\n",
        "    raw_en = load_raw_data(path_en)\n",
        "    raw_vi = load_raw_data(path_vi)\n",
        "\n",
        "    assert len(raw_en) == len(raw_vi), \"Lỗi lệch dòng!\"\n",
        "\n",
        "    # --- [QUAN TRỌNG] CẮT DỮ LIỆU ĐỂ TEST ---\n",
        "    if TEST_MODE_LIMIT is not None:\n",
        "        print(f\"\\n  CHẾ ĐỘ TEST: Đang cắt dữ liệu xuống còn {TEST_MODE_LIMIT} câu...\")\n",
        "        raw_en = raw_en[:TEST_MODE_LIMIT]\n",
        "        raw_vi = raw_vi[:TEST_MODE_LIMIT]\n",
        "    # ----------------------------------------\n",
        "\n",
        "    print(f\"-> Tổng số câu sử dụng: {len(raw_en)}\")\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # BƯỚC 1: CHIA TẬP TRAIN / VAL\n",
        "    # -----------------------------------------\n",
        "    print(f\"\\n--- Chia tập Train/Val (Ratio: {VAL_SPLIT_RATIO}) ---\")\n",
        "    en_train, en_val, vi_train, vi_val = train_test_split(\n",
        "        raw_en, raw_vi, test_size=VAL_SPLIT_RATIO, random_state=SEED\n",
        "    )\n",
        "    print(f\"-> Train: {len(en_train)} | Val: {len(en_val)}\")\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # BƯỚC 2: LOAD MODEL SENTENCEPIECE\n",
        "    # -----------------------------------------\n",
        "    print(\"\\n--- Load SPM Models ---\")\n",
        "    sp_en = spm.SentencePieceProcessor(); sp_en.load(os.path.join(DATA_DIR, 'spm_en.model'))\n",
        "    sp_vi = spm.SentencePieceProcessor(); sp_vi.load(os.path.join(DATA_DIR, 'spm_vi.model'))\n",
        "\n",
        "    print(f\"-> Vocab Size: {sp_en.get_piece_size()} / {sp_vi.get_piece_size()}\")\n",
        "\n",
        "\n",
        "    # -----------------------------------------\n",
        "    # BƯỚC 3: TẠO DATASET & DATALOADER\n",
        "    # -----------------------------------------\n",
        "    print(\"\\n--- Tạo DataLoader ---\")\n",
        "    train_dataset = TranslationDatasetSPM(en_train, vi_train, sp_en, sp_vi, max_len=MAX_SEQ_LEN)\n",
        "    val_dataset = TranslationDatasetSPM(en_val, vi_val, sp_en, sp_vi, max_len=MAX_SEQ_LEN)\n",
        "\n",
        "    collate_fn = MyCollateSPM(pad_idx=PAD_IDX)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    print(f\"-> Train Batches: {len(train_loader)}\")\n",
        "    print(f\"-> Val Batches:   {len(val_loader)}\")\n",
        "\n",
        "    return train_loader, val_loader"
      ],
      "metadata": {
        "id": "CVISO5rrNL4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import math\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import copy\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    train_loader, val_loader = process_data_pipeline_for_test()\n",
        "    print(f\"-> Train/Val Loader đã sẵn sàng (Batch Size: {BATCH_SIZE})\")\n",
        "\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    ## BƯỚC 1.3: KHỞI TẠO MÔ HÌNH VÀ CHUYỂN DEVICE\n",
        "    # ----------------------------------------------------\n",
        "    print(\"3. Đang khởi tạo mô hình Transformer...\")\n",
        "\n",
        "    # Gọi hàm make_model đã được định nghĩa\n",
        "    model = make_model(VOCAB_SIZE_SPM, VOCAB_SIZE_SPM, N=N_LAYERS, d_model=D_MODEL, h=N_HEADS, d_ff=D_FF, dropout=DROPOUT_RATE)\n",
        "    model.to(DEVICE)\n",
        "    print(f\"-> Mô hình đã được khởi tạo và chuyển sang {DEVICE}. Tổng tham số: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    ## BƯỚC 1.4: KHỞI TẠO CÁC THÀNH PHẦN TRAINING\n",
        "    # ----------------------------------------------------\n",
        "    print(\"4. Đang thiết lập Loss Function và Optimizer...\")\n",
        "\n",
        "    # 1. Base Optimizer (AdamW được khuyến nghị)\n",
        "    base_optimizer = optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-4)\n",
        "\n",
        "    # 2. Noam Scheduler (Wrapped Optimizer)\n",
        "    optimizer = NoamOpt(model_size=D_MODEL, warmup_steps=WARMUP_STEPS, optimizer=base_optimizer)\n",
        "\n",
        "    # 3. Criterion (Label Smoothing)\n",
        "    # criterion = LabelSmoothing(size=VOCAB_SIZE_VI, padding_idx=PAD_IDX, smoothing=LABEL_SMOOTHING).to(DEVICE)\n",
        "    # Do KLDivLoss hoạt động trên Log-Probs, ta dùng nn.CrossEntropyLoss cho mục đích\n",
        "    # tính loss trên Validation set (VALID_CRITERION)\n",
        "\n",
        "    # Khởi tạo Criterion mới: Dùng KLDivLoss/LabelSmoothing cho Train\n",
        "    criterion_train = LabelSmoothing(size=VOCAB_SIZE_SPM, padding_idx=PAD_IDX, smoothing=LABEL_SMOOTHING).to(DEVICE)\n",
        "\n",
        "    # Khởi tạo Criterion cho Validation (CrossEntropyLoss đơn thuần)\n",
        "    criterion_val = nn.CrossEntropyLoss(ignore_index=PAD_IDX, reduction='sum').to(DEVICE)\n",
        "\n",
        "    print(f\"-> Optimizer (Noam) và Loss (Label Smoothing {LABEL_SMOOTHING}) đã sẵn sàng.\")\n",
        "\n",
        "\n",
        "    # ----------------------------------------------------\n",
        "    ## BƯỚC 1.5: CHẠY VÒNG LẶP HUẤN LUYỆN CHÍNH\n",
        "    # ----------------------------------------------------\n",
        "    print(\"5. Bắt đầu Vòng Lặp Huấn Luyện...\")\n",
        "\n",
        "    # Chạy hàm training chính đã được sửa lỗi và tối ưu\n",
        "    run_full_training_pipeline(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion_train=criterion_train, # Cần sửa lại signature của hàm training\n",
        "        criterion_val=criterion_val,     # Truyền vào 2 criterion khác nhau\n",
        "        optimizer=optimizer,\n",
        "        pad_idx=PAD_IDX,\n",
        "        n_epochs=EPOCHS,\n",
        "        clip_norm=GRAD_CLIP\n",
        "    )\n",
        "\n",
        "\n",
        "# --- BƯỚC 2: KHỐI CHẠY CHƯƠNG TRÌNH ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Đảm bảo CHECKPOINT_DIR tồn tại\n",
        "    if not os.path.exists(CHECKPOINT_DIR):\n",
        "        os.makedirs(CHECKPOINT_DIR)\n",
        "\n",
        "    # Gọi hàm chính\n",
        "    #main()\n",
        "    print(\"\\n--- ĐÃ HOÀN TẤT SETUP HÀM MAIN ---\")\n",
        "    print(\"Bây giờ bạn chỉ cần gọi hàm main() để bắt đầu training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJPWy0k8JUzp",
        "outputId": "1e33c59a-6004-40fb-ef2d-41e96f4dcb75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ĐÃ HOÀN TẤT SETUP HÀM MAIN ---\n",
            "Bây giờ bạn chỉ cần gọi hàm main() để bắt đầu training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sentencepiece as spm\n",
        "import os\n",
        "import math\n",
        "import copy # Vẫn cần copy và math cho việc khởi tạo model\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. CẤU HÌNH & LOAD SPM (Khởi tạo các hằng số cần thiết)\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "print(\"--- 1. LOADING SPM MODELS ---\")\n",
        "try:\n",
        "    sp_en = spm.SentencePieceProcessor()\n",
        "    sp_en.load(os.path.join(DATA_DIR, 'spm_en.model'))\n",
        "    sp_vi = spm.SentencePieceProcessor()\n",
        "    sp_vi.load(os.path.join(DATA_DIR, 'spm_vi.model'))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"LỖI LOAD SPM: {e}. Vui lòng kiểm tra lại đường dẫn.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "try:\n",
        "    # 1. Khởi tạo model rỗng dựa trên kiến trúc đã định nghĩa\n",
        "    model = make_model(VOCAB_SIZE_SPM, VOCAB_SIZE_SPM, N=N_LAYERS, d_model=D_MODEL, h=N_HEADS).to(DEVICE)\n",
        "    print(\"-> Đã khởi tạo cấu trúc mô hình (cần load trọng số).\")\n",
        "except NameError:\n",
        "    print(\"\\n--- LỖI QUAN TRỌNG ---\")\n",
        "    print(\"  Lớp hoặc hàm `make_model` CHƯA ĐƯỢC ĐỊNH NGHĨA. Vui lòng chạy lại đoạn code định nghĩa kiến trúc Transformer trước đó.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3. TẢI TRỌNG SỐ TỪ CHECKPOINT VÀ HÀM DECODE\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "print(\"\\n--- 3. TẢI TRỌNG SỐ VÀ SETUP INFERENCE ---\")\n",
        "\n",
        "# 1. Tải trọng số\n",
        "checkpoint_path = os.path.join(CHECKPOINT_DIR, 'latest_checkpoint.pth')\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.to(DEVICE)\n",
        "else:\n",
        "    print(f\"  CẢNH BÁO: Không tìm thấy checkpoint tại {checkpoint_path}.\")\n",
        "    print(\"Model sẽ chạy với trọng số ngẫu nhiên hoặc đã được khởi tạo trước đó.\")\n",
        "\n",
        "# Hàm hỗ trợ cho Masking\n",
        "def subsequent_mask(size):\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
        "    return subsequent_mask == 0\n",
        "\n",
        "def make_std_mask_decode(tgt, pad_idx):\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(-2)\n",
        "    tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data).to(DEVICE)\n",
        "    return tgt_mask\n",
        "\n",
        "# Hàm dịch (Greedy Decoding)\n",
        "def simple_greedy_decode(model, src, src_mask, max_len, sp_vi):\n",
        "    model.eval()\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(sp_vi.bos_id()).type_as(src.data).to(DEVICE)\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        tgt_mask = make_std_mask_decode(ys, sp_vi.pad_id())\n",
        "        out = model.decode(memory, src_mask, ys, tgt_mask)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word).to(DEVICE)], dim=1)\n",
        "\n",
        "        if next_word == sp_vi.eos_id():\n",
        "            break\n",
        "\n",
        "    # Giải mã và trả về kết quả\n",
        "    translated_ids = ys.squeeze(0).tolist()[1:]\n",
        "    if sp_vi.eos_id() in translated_ids:\n",
        "        translated_ids = translated_ids[:translated_ids.index(sp_vi.eos_id())]\n",
        "\n",
        "    return sp_vi.decode_ids(translated_ids)\n",
        "\n",
        "\n",
        "def beam_search_decode(model, src, src_mask, max_len, sp_vi, beam_size=12):\n",
        "    model.eval()\n",
        "    device = src.device\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "\n",
        "    # (sequence, log_prob)\n",
        "    beams = [(torch.tensor([[sp_vi.bos_id()]], device=device), 0.0)]\n",
        "    completed = []\n",
        "\n",
        "    for _ in range(max_len - 1):\n",
        "        new_beams = []\n",
        "\n",
        "        for seq, score in beams:\n",
        "            if seq[0, -1].item() == sp_vi.eos_id():\n",
        "                completed.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            tgt_mask = make_std_mask_decode(seq, sp_vi.pad_id())\n",
        "            out = model.decode(memory, src_mask, seq, tgt_mask)\n",
        "            logits = model.generator(out[:, -1])\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            topk_log_probs, topk_ids = torch.topk(log_probs, beam_size, dim=-1)\n",
        "\n",
        "            for k in range(beam_size):\n",
        "                next_id = topk_ids[0, k].item()\n",
        "                next_score = score + topk_log_probs[0, k].item()\n",
        "\n",
        "                next_seq = torch.cat(\n",
        "                    [seq, torch.tensor([[next_id]], device=device)], dim=1\n",
        "                )\n",
        "\n",
        "                new_beams.append((next_seq, next_score))\n",
        "\n",
        "        # Giữ beam tốt nhất\n",
        "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
        "\n",
        "        if len(completed) >= beam_size:\n",
        "            break\n",
        "\n",
        "    # Nếu chưa có eos → lấy beam tốt nhất\n",
        "    if len(completed) == 0:\n",
        "        completed = beams\n",
        "\n",
        "    best_seq = max(completed, key=lambda x: x[1])[0]\n",
        "\n",
        "    # Bỏ BOS và EOS\n",
        "    translated_ids = best_seq.squeeze(0).tolist()[1:]\n",
        "    if sp_vi.eos_id() in translated_ids:\n",
        "        translated_ids = translated_ids[:translated_ids.index(sp_vi.eos_id())]\n",
        "\n",
        "    return sp_vi.decode_ids(translated_ids)\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4. TEST CÂU ĐƠN GIẢN\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_simple_test(input_sentence):\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Tokenize và chuẩn bị Input\n",
        "    src_ids = sp_en.encode_as_ids(input_sentence.lower())\n",
        "    src_ids = [BOS_IDX] + src_ids + [EOS_IDX]\n",
        "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "    src_mask = (src_tensor != PAD_IDX).unsqueeze(-2).to(DEVICE)\n",
        "\n",
        "    # 2. Dịch\n",
        "    with torch.no_grad():\n",
        "        translation = beam_search_decode(model, src_tensor, src_mask, 100, sp_vi)\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"EN Input: {input_sentence}\")\n",
        "    print(f\"VI Output: {translation}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "\n",
        "print(\"\\n--- 4. THỬ NGHIỆM DỊCH (Sau khi load model) ---\")\n",
        "#run_simple_test(\"I am happy to test the translation model.\")\n",
        "#run_simple_test(\"What is your name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Onv7DE-flPvr",
        "outputId": "7b053e9e-5af4-46af-d8b6-6de4b9586047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. LOADING SPM MODELS ---\n",
            "-> Đã khởi tạo cấu trúc mô hình (cần load trọng số).\n",
            "\n",
            "--- 3. TẢI TRỌNG SỐ VÀ SETUP INFERENCE ---\n",
            "\n",
            "--- 4. THỬ NGHIỆM DỊCH (Sau khi load model) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Đánh giá"
      ],
      "metadata": {
        "id": "BPDPKHT-EU9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-28MhvJMRRO",
        "outputId": "91d1f61b-629b-41f4-e5bb-8c88728c2d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (6.0.2)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/104.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "import os, math, json\n",
        "import sentencepiece as spm\n",
        "import sacrebleu\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sp_en = spm.SentencePieceProcessor()\n",
        "sp_vi = spm.SentencePieceProcessor()\n",
        "sp_en.load(os.path.join(DATA_DIR, 'spm_en.model'))\n",
        "sp_vi.load(os.path.join(DATA_DIR, 'spm_vi.model'))\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    mask = torch.triu(torch.ones(size, size, device=DEVICE), diagonal=1).bool()\n",
        "    return (~mask).unsqueeze(0)\n",
        "\n",
        "def make_std_mask_decode(tgt, pad_idx):\n",
        "    tgt_mask = (tgt != pad_idx).unsqueeze(-2)\n",
        "    tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1))\n",
        "    return tgt_mask\n",
        "\n",
        "\n",
        "model = make_model(\n",
        "    VOCAB_SIZE_SPM,\n",
        "    VOCAB_SIZE_SPM,\n",
        "    N=6,\n",
        "    d_model=512,\n",
        "    h=8\n",
        ").to(DEVICE)\n",
        "\n",
        "checkpoint = torch.load(\n",
        "    os.path.join(CHECKPOINT_DIR, 'best_model.pth'),\n",
        "    map_location=DEVICE\n",
        ")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(\"Model & SentencePiece loaded successfully\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. DATASET CHO PPL\n",
        "# ==============================================================================\n",
        "class TestDatasetPPL(Dataset):\n",
        "    def __init__(self, raw_en, raw_vi):\n",
        "        self.data = list(zip(raw_en, raw_vi))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        en, vi = self.data[idx]\n",
        "        src = [BOS_IDX] + sp_en.encode_as_ids(en.lower()) + [EOS_IDX]\n",
        "        tgt = [BOS_IDX] + sp_vi.encode_as_ids(vi.lower()) + [EOS_IDX]\n",
        "        return torch.tensor(src), torch.tensor(tgt)\n",
        "\n",
        "class CollatePPL:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        srcs, tgts = zip(*batch)\n",
        "        srcs = pad_sequence(srcs, batch_first=True, padding_value=self.pad_idx)\n",
        "        tgts = pad_sequence(tgts, batch_first=True, padding_value=self.pad_idx)\n",
        "        return srcs.to(DEVICE), tgts.to(DEVICE)\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. PERPLEXITY\n",
        "# ==============================================================================\n",
        "def calculate_ppl(loader):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, reduction='sum')\n",
        "    total_loss, total_tokens = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in tqdm(loader, desc=\"Calculating PPL\"):\n",
        "            tgt_in = tgt[:, :-1]\n",
        "            tgt_y = tgt[:, 1:]\n",
        "\n",
        "            src_mask = (src != PAD_IDX).unsqueeze(-2)\n",
        "            tgt_mask = make_std_mask_decode(tgt_in, PAD_IDX)\n",
        "\n",
        "            logits = model(src, tgt_in, src_mask, tgt_mask)\n",
        "\n",
        "            loss = criterion(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                tgt_y.contiguous().view(-1)\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_tokens += (tgt_y != PAD_IDX).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    return avg_loss, math.exp(avg_loss)\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. BEAM SEARCH (1 câu)\n",
        "# ==============================================================================\n",
        "def beam_translate(src_sentence):\n",
        "    src_ids = [BOS_IDX] + sp_en.encode_as_ids(src_sentence.lower()) + [EOS_IDX]\n",
        "    src = torch.tensor(src_ids).unsqueeze(0).to(DEVICE)\n",
        "    src_mask = (src != PAD_IDX).unsqueeze(-2)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "\n",
        "    beams = [(0.0, torch.tensor([[BOS_IDX]], device=DEVICE))]\n",
        "\n",
        "    for _ in range(MAX_LEN_DECODE):\n",
        "        candidates = []\n",
        "\n",
        "        for score, seq in beams:\n",
        "            if seq[0, -1].item() == EOS_IDX:\n",
        "                candidates.append((score, seq))\n",
        "                continue\n",
        "\n",
        "            tgt_mask = make_std_mask_decode(seq, PAD_IDX)\n",
        "            out = model.decode(memory, src_mask, seq, tgt_mask)\n",
        "            log_prob = F.log_softmax(model.generator(out[:, -1]), dim=-1)\n",
        "\n",
        "            topk = torch.topk(log_prob, BEAM_SIZE)\n",
        "\n",
        "            for i in range(BEAM_SIZE):\n",
        "                new_score = score + topk.values[0, i].item()\n",
        "                new_seq = torch.cat(\n",
        "                    [seq, topk.indices[0, i].view(1, 1)], dim=1\n",
        "                )\n",
        "                candidates.append((new_score, new_seq))\n",
        "\n",
        "        beams = sorted(\n",
        "            candidates,\n",
        "            key=lambda x: x[0] / (x[1].size(1) ** 0.7),\n",
        "            reverse=True\n",
        "        )[:BEAM_SIZE]\n",
        "\n",
        "        if beams[0][1][0, -1].item() == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    best_seq = beams[0][1][0, 1:].tolist()\n",
        "    if EOS_IDX in best_seq:\n",
        "        best_seq = best_seq[:best_seq.index(EOS_IDX)]\n",
        "\n",
        "    return sp_vi.decode_ids(best_seq)\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. BLEU + REPORT\n",
        "# ==============================================================================\n",
        "def evaluate():\n",
        "    raw_en = load_raw_data(TEST_EN_PATH)\n",
        "    raw_vi = load_raw_data(TEST_VI_PATH)\n",
        "\n",
        "    ppl_ds = TestDatasetPPL(raw_en, raw_vi)\n",
        "    ppl_loader = DataLoader(\n",
        "        ppl_ds,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        collate_fn=CollatePPL(PAD_IDX)\n",
        "    )\n",
        "\n",
        "    loss, ppl = calculate_ppl(ppl_loader)\n",
        "\n",
        "    print(\"\\n Translating with Beam Search...\")\n",
        "    hypotheses = [beam_translate(s) for s in tqdm(raw_en)]\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(hypotheses, [raw_vi])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\" FINAL EVALUATION RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Perplexity (PPL): {ppl:.2f}\")\n",
        "    print(f\"BLEU Score: {bleu.score:.2f}\")\n",
        "    print(\"=\"*60)\n",
        "    print(bleu.format())\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return ppl, bleu.score\n",
        "\n",
        "# ==============================================================================\n",
        "# 8. RUN\n",
        "# ==============================================================================\n",
        "evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w702IWhiEUlj",
        "outputId": "32dcc4c7-64c6-4b66-da77-3e30de7aa4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model & SentencePiece loaded successfully\n",
            "-> Đang đọc file: tst2013.en...\n",
            "-> Đang đọc file: tst2013.vi...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating PPL: 100%|██████████| 20/20 [00:03<00:00,  5.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔁 Translating with Beam Search...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1268/1268 [28:43<00:00,  1.36s/it]\n",
            "WARNING:sacrebleu:That's 100 lines that end in a tokenized period ('.')\n",
            "WARNING:sacrebleu:It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "WARNING:sacrebleu:If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "📊 FINAL EVALUATION RESULTS\n",
            "============================================================\n",
            "Test Loss: 2.0334\n",
            "Test Perplexity (PPL): 7.64\n",
            "BLEU Score: 26.37\n",
            "============================================================\n",
            "BLEU = 26.37 59.3/33.9/20.7/12.9 (BP = 0.975 ratio = 0.975 hyp_len = 32905 ref_len = 33738)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7.639712025620963, 26.36685262435577)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E_Z7zv7FlqEs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}